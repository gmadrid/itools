Goal: To find dups and near dups in a large corpus of image files.

1) Scan images and compute hashes for them. (sha2(256), ahash, dhash, phash)

   - Don't recompute existing hashes.

   - If a dup image (by sha2) is found, don't recompute hashes.

   - Ideally, we will not re-read image files for each hash.

   - Also, ideally, we will read the images on a single thread, but do
     hash computation in parallel.

2) Save scanned file data so that it doesn't need to be recomputed.

   - periodically dump save to file so that partial scans aren't wasted.

3) Specify how to find dups

   - exact matches

   - bit difference by hash (or all)

4) Output by

   - text

   - json

   - "open" in Preview


// Basic structure of the Hasher.
// Input is a list of files.
// Output is a channel of fully-populated FileInfo objects.
  FileWalker -> FileReader -> Sha2Hasher  ->         ---> DataAggregator
                              ImageReader -> AHasher -^
			                  +> DHasher -^
					  +> PHasher -^


// PersistedCache
// 1) take channel of FileInfo, and put them into a HashMap (by filename?)
// 2) periodically, save this HashMap into a persisted file (json/serde?)
// 3) When creating the PersistedCache, specify a filename for loading.
// 4) Also, specify the name of the output file on creation.
// 5) If inputname == outputname, nothing bad should happen.

// Search
--distance,  -d  Bit difference (only 0 is valid for sha2)
--hash_type, -t  sha2|mean|grad|dct